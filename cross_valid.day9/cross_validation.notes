# ğŸ“Œ Cross Validation & GridSearchCV Notes (Smart Folder Version)

## 1ï¸âƒ£ Concepts

**Hyperparameter vs Parameter:**

* **Hyperparameter:** Model ki settings, training se pehle hum set karte. Example: `C`, `solver`, `max_depth`.
* **Parameter:** Model data se training ke dauraan seekhta. Example: Logistic Regression ke weights/coefficients.
* **Trick:** Hyper = hum set karte, Parameter = model seekhta.

**Cross Validation (CV):**

* Goal: Model ka performance accurately measure karna.
* Process: Data ko `k folds` me divide â†’ 1 fold test, rest train â†’ repeat k times â†’ average score.
* Trick: "CV = model ko multiple times test karna without leaking test data"

**GridSearchCV:**

* Goal: Best hyperparameters choose karna.
* Process: Define hyperparameter grid â†’ har combination pe CV â†’ best combo select â†’ final model.
* Trick: "GridSearch = sab possible settings try â†’ best choose"

## 2ï¸âƒ£ Hyperparameter Grid Example

```python
param_grid = {
    'C': [0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}
```

| C   | solver    |
| --- | --------- |
| 0.1 | liblinear |
| 0.1 | lbfgs     |
| 1   | liblinear |
| 1   | lbfgs     |
| 10  | liblinear |
| 10  | lbfgs     |

* Each row = **hyperparameter combination**
* GridSearchCV tries **all combinations** using CV â†’ selects best

## 3ï¸âƒ£ Python Step-by-Step Example

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model
model = LogisticRegression(max_iter=200)

# 5-Fold CV
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
print("CV Scores:", cv_scores)
print("Average CV Score:", cv_scores.mean())

# GridSearchCV
param_grid = {'C':[0.1,1,10], 'solver':['liblinear','lbfgs']}
grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best CV Score:", grid.best_score_)

# Test Evaluation
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

## 4ï¸âƒ£ Flow Diagram (Simple)

```
Data â†’ Train/Test Split
       |
       V
   Train Data
       |
       +--> Cross Validation (k-fold) --> Avg CV Score
       |
       +--> GridSearchCV (All hyperparam combinations) --> Best params & CV Score
       |
       V
Best Model --> Test Data --> Accuracy, Confusion Matrix, Classification Report
```

## 5ï¸âƒ£ Tips & Tricks

* **CV:** k=5 or 10 common; higher k = more stable estimate, slower
* **GridSearchCV:** Always use CV to avoid overfitting
* **Metrics:** Classification â†’ accuracy, f1-score; Regression â†’ RMSE, RÂ²
* **grid.best_estimator_** â†’ Already trained final model
* **RandomizedSearchCV** â†’ Use if hyperparameter grid is huge

**Real Life Analogy:** Making Tea â˜•

* Hyperparameter = settings (chai patti, sugar, steeping time)
* Parameter = actual flavor / sweetness after steeping

