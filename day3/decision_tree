## Decision Tree & Overfitting Notes

### 1️⃣ Decision Tree

* Tree-like structure to split data based on questions/conditions.
* Each node = question (e.g., Age > 30?)
* Each branch = answer (Yes/No)
* Leaf node = final decision/class (e.g., Will Buy/Not Buy)

**Example:**

| Age | Income | Buy? |
| --- | ------ | ---- |
| 25  | High   | No   |
| 35  | High   | Yes  |
| 40  | Low    | Yes  |
| 20  | Low    | No   |

Decision Tree:

```
      Age > 30?
       /    \
     Yes     No
     /        \
  Income>50k?   No
   /    \
  Yes    No
 Yes     Yes
```

### 2️⃣ Overfitting

* Model **memorizes training data** instead of learning patterns.
* Deep tree = high training accuracy, poor test accuracy.
* Example: Adding too many conditions (Age, Income, Gender, Hobby, etc.)
* Overfitting = "Data ka student, teacher ki book se jyada yaad kar raha hai, concept samjhe bina"

### 3️⃣ Solutions to Overfitting

1. Limit **max depth** of the tree
2. Set **minimum samples per leaf/node**
3. **Pruning** extra branches (noise-based)
4. Use **ensemble methods** like Random Forest or Gradient Boosting

---

**Tips:**

* Decision Tree = easy to understand, visualize.
* Overfitting = always check with **unseen/test data**.
* Ensemble methods = reduce overfitting, improve generalization.

sklearn -> Classical ML models & utilities -> Load dataset -> Preprocess -> Train model -> Predict -> Evaluate
sklearn.datasets.load_iris -> Load standard ML dataset (Iris) -> Dataset -> Features(X) & Labels(y)
sklearn.tree.DecisionTreeClassifier -> Build classification tree -> X_train, y_train -> Fit -> Predict
sklearn.tree.plot_tree -> Visualize decision tree -> Fitted tree -> plot -> inspect splits
sklearn.model_selection.train_test_split -> Split data into train & test -> X, y -> train_test_split -> X_train, X_test, y_train, y_test
matplotlib.pyplot -> Plot graphs & visualizations -> Data/model -> plt.plot/plt.show -> Analyze

